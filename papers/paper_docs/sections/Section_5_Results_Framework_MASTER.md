# 5. Results Framework

This section outlines the expected results structure and analysis approach for the planned evaluation study described in Section 4. The study will be conducted following paper acceptance, with results to be presented at the conference and in subsequent publications.

## 5.1 Quantitative Analysis Plan

The study will generate quantitative data across three comparison conditions using the leave-one-out cross-validation approach on 20 NSPE engineering ethics cases. Primary quantitative measures will include participant preference ratings, reasoning quality scores, and accuracy assessments.

### Comparative Performance Metrics

**Participant Preference Analysis**: Direct comparison ratings between ProEthica and baseline system outputs will be analyzed using chi-square tests to determine significant preference patterns. Expected results will show preference distributions across the three evaluation conditions (conclusion comparison, discussion comparison, and combined analysis).

**Reasoning Quality Assessment**: Seven-point Likert scale ratings for logical coherence, systematic principle application, and structured argumentation will be analyzed using t-tests to identify significant quality differences between systems. Effect sizes will be calculated using Cohen's d to assess practical significance of observed differences.

**Professional Alignment Accuracy**: Binary assessments of alignment with original NSPE Board determinations will be analyzed using McNemar's test for paired data to evaluate systematic differences in accuracy rates between ProEthica and baseline approaches.

### Statistical Analysis Framework

All quantitative analyses will employ appropriate statistical tests with Bonferroni correction for multiple comparisons. Confidence intervals (95%) will be calculated for all effect estimates to provide reliable bounds for performance differences. Sample size calculations indicate that 60-80 participants will provide adequate statistical power (Î² = 0.80) for detecting medium effect sizes (d = 0.5) in comparative analyses.

Missing data will be handled using listwise deletion for complete case analysis, with sensitivity analyses conducted to assess impact of incomplete responses on primary findings.

## 5.2 Qualitative Analysis Plan

Qualitative analysis will focus on participant feedback regarding reasoning comprehensibility, persuasiveness, and professional appropriateness of system outputs across both ProEthica and baseline conditions.

### Thematic Content Analysis

**Reasoning Pattern Identification**: Participant comments will be analyzed to identify recurring themes regarding reasoning quality, principle application, and professional alignment. Thematic analysis will follow established qualitative research protocols with independent coding by multiple reviewers to ensure reliability.

**Professional Appropriateness Assessment**: Qualitative feedback regarding adherence to professional reasoning patterns and standards will be categorized and analyzed to understand how ontological constraints affect perceived professional alignment of system outputs.

**Usability and Clarity Evaluation**: Participant responses regarding reasoning comprehensibility and accessibility will be analyzed to assess whether structured constraint mechanisms improve or impair communication effectiveness for non-expert audiences.

### Integration with Quantitative Findings

Qualitative themes will be integrated with quantitative results to provide comprehensive understanding of how ontological constraints affect both measured performance and perceived quality of ethical reasoning. Mixed-methods analysis will identify convergent and divergent findings across quantitative and qualitative data sources.

## 5.3 System Performance Validation

Beyond participant evaluation, the study will collect system performance data to validate technical capabilities and operational characteristics of the ProEthica implementation.

### Constraint Compliance Monitoring

**Ontological Consistency Tracking**: For each processed case, the system will log constraint compliance rates, ontological consultation frequency, and validation success rates. These metrics will provide evidence of systematic constraint enforcement throughout reasoning processes.

**Processing Efficiency Measurement**: Response time, computational resource utilization, and scalability characteristics will be measured across all evaluation cases to assess practical deployment feasibility.

### Precedent Analysis Validation

**Relevance Scoring Accuracy**: Multi-metric precedent identification will be validated through expert assessment of case similarity and analogical appropriateness. Human experts will evaluate whether system-identified precedents represent genuine analogical matches for target cases.

**FIRAC Structure Compliance**: Systematic evaluation of reasoning organization according to Facts, Issues, Rules, Analysis, Conclusion framework will assess whether ontological constraints improve structured reasoning consistency.

## 5.4 Expected Outcomes and Implications

Based on the system architecture and preliminary demonstrations, several outcome patterns are anticipated from the planned evaluation study.

### Primary Hypothesis Validation

The central hypothesis that ontology-constrained prompts will produce more reasonable, persuasive, and aligned ethical reasoning is expected to be supported through systematic preference patterns, higher quality ratings, and improved accuracy alignment with expert determinations.

**Reasoning Quality Enhancement**: ProEthica outputs are expected to demonstrate superior logical coherence and systematic principle application compared to baseline approaches, reflecting the structured constraint mechanisms and precedent analysis capabilities.

**Professional Alignment Improvement**: Higher accuracy rates in predicting NSPE Board determinations are anticipated, demonstrating that ontological constraints can improve alignment with established professional ethical reasoning patterns.

### Secondary Outcome Expectations

**Persuasiveness and Clarity**: Structured reasoning organization and systematic precedent integration are expected to enhance argument persuasiveness and communication effectiveness, particularly for audiences familiar with professional ethical reasoning patterns.

**Constraint Mechanism Validation**: System performance data should demonstrate consistent ontological constraint enforcement without significant computational overhead, supporting practical deployment feasibility.

### Interpretation Framework

Results will be interpreted relative to the three gaps identified in Section 1.2: formal constraint mechanisms, analogical precedent reasoning, and bidirectional validation. Successful validation of the primary hypothesis will provide evidence that systematic ontological integration can address these fundamental limitations in current LLM-based ethical reasoning approaches.

Negative or mixed results will be analyzed to identify specific limitations in the ontological constraint approach and guide future system development. Particular attention will be paid to cases where constraint mechanisms may limit reasoning flexibility or where precedent analysis fails to identify appropriate analogical relationships.

## 5.5 Publication and Dissemination Plan

Results from the completed study will be disseminated through multiple channels to maximize impact and enable replication by other researchers.

### Conference Presentation

Primary findings will be presented at the conference venue, with emphasis on practical implications for AI deployment in professional contexts and methodological contributions to ontology-constrained reasoning evaluation.

### Extended Publication

Complete results will be prepared for journal submission with detailed statistical analyses, comprehensive qualitative findings, and expanded discussion of implications for professional AI system development.

### Reproducibility Support

All evaluation materials, statistical analysis code, and system implementation details will be made available to support replication studies and enable comparison with alternative constraint mechanisms or professional domains.

The evaluation framework provides systematic assessment of ontology-constrained reasoning effectiveness while maintaining scientific rigor and practical relevance for professional AI deployment contexts.
